{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58048d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-Time Facial Emotion Recognition with Emoji UI (20 Steps)\n",
    "\n",
    "## Step 1: Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from datetime import datetime\n",
    "import tkinter as tk\n",
    "\n",
    "from tkinter import messagebox, Menu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155b99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Configure Encoding\n",
    "\n",
    "try:\n",
    "    sys.stdout.reconfigure(encoding='utf-8')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14584724",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Suppress TensorFlow Warnings\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c77b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Set Paths for Emoji and Save Directory\n",
    "\n",
    "emoji_dir = r\"C:\\Users\\moham\\Downloads\\Emotion_Recognition_Project\\Real-time-Facial-Emotion-Recognition-using-OpenCV-and-Deepface\\Emoji\"\n",
    "save_path = r\"C:\\Users\\moham\\Downloads\\Emotion_Captures\"\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f72b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Define Emotion-to-Emoji Mapping\n",
    "\n",
    "emotion_to_emoji = {\n",
    "    'angry': 'Angry.gif',\n",
    "    'disgust': 'Disgust.gif',\n",
    "    'fear': 'Fear.gif',\n",
    "    'happy': 'Smile.gif',\n",
    "    'sad': 'Sad face.gif',\n",
    "    'surprise': 'Surprise.gif',\n",
    "    'neutral': 'Neutral.gif'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a364fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Load Emojis for Animation and Bar Graph\n",
    "\n",
    "emoji_gifs = {}\n",
    "emoji_icons = {}\n",
    "\n",
    "for emotion, filename in emotion_to_emoji.items():\n",
    "    path = os.path.join(emoji_dir, filename)\n",
    "    try:\n",
    "        frames = imageio.mimread(path)\n",
    "        emoji_gifs[emotion] = [cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR) for frame in frames]\n",
    "        emoji_icons[emotion] = cv2.resize(emoji_gifs[emotion][0], (18, 18))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Error loading emoji for '{emotion}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba0cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Load Haar Cascade for Face Detection\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emotion detection\n",
    "\n",
    "def detect_emotions():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    emoji_frame_index = 0\n",
    "    dominant_emotion = None\n",
    "    emotion_scores = None\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "        face_count = len(faces)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_roi = rgb[y:y + h, x:x + w]\n",
    "\n",
    "            try:\n",
    "                result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                dominant_emotion = result[0]['dominant_emotion'].lower()\n",
    "                emotion_scores = result[0]['emotion']\n",
    "\n",
    "                # Draw green box with emotion\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, dominant_emotion.upper(), (x, y - 10), font, 0.7, (0, 255, 0), 2)\n",
    "            except Exception as e:\n",
    "                print(\"[WARNING] Emotion detection error:\", e)\n",
    "            break  # Only process the first face\n",
    "\n",
    "        # === LEFT PANEL CREATION ===\n",
    "        panel_width = 240\n",
    "        panel = np.ones((frame.shape[0], panel_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Top emoji gif\n",
    "        if dominant_emotion in emoji_gifs:\n",
    "            emoji_frames = emoji_gifs[dominant_emotion]\n",
    "            emoji_img = emoji_frames[emoji_frame_index % len(emoji_frames)]\n",
    "            emoji_frame_index += 1\n",
    "            emoji_resized = cv2.resize(emoji_img, (144, 144))\n",
    "            panel[10:10 + 144, 48:48 + 144] = emoji_resized\n",
    "            cv2.putText(panel, dominant_emotion.upper(), (65, 170), font, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        # Bar chart with emotion values\n",
    "        if emotion_scores:\n",
    "            sorted_emotions = sorted(emotion_scores.items(), key=lambda item: -item[1])\n",
    "            bar_start_y = 200\n",
    "            bar_height = 18\n",
    "            spacing = 6\n",
    "            max_bar_width = 100\n",
    "\n",
    "            for idx, (emo, val) in enumerate(sorted_emotions):\n",
    "                y = bar_start_y + idx * (bar_height + spacing)\n",
    "\n",
    "                # Draw icon\n",
    "                if emo in emoji_icons:\n",
    "                    panel[y:y + 24, 10:34] = emoji_icons[emo]\n",
    "\n",
    "                # Draw emotion label and bar\n",
    "                bar_len = int((val / 100) * max_bar_width)\n",
    "                cv2.rectangle(panel, (40, y + 5), (140, y + 5 + bar_height), (230, 230, 230), -1)\n",
    "                cv2.rectangle(panel, (40, y + 5), (40 + bar_len, y + 5 + bar_height), (0, 100, 255), -1)\n",
    "                cv2.putText(panel, f\"{emo.upper()} : {int(val)}%\", (145, y + 20), font, 0.4, (0, 0, 0), 1)\n",
    "\n",
    "        # Face Count\n",
    "        cv2.putText(frame, f\"Faces: {face_count}\", (10, 30), font, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # === Combine Left Panel and Webcam Frame ===\n",
    "        combined = np.hstack((panel, frame))\n",
    "        cv2.imshow(\"Real-Time Emotion Detection\", combined)\n",
    "\n",
    "        key = cv2.waitKey(100) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cae6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emotion recognition app(only detect the face)\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import Toplevel, Label, Menu, Button\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "def show_manual():\n",
    "    manual_win = Toplevel(root)\n",
    "    manual_win.title(\"User Manual\")\n",
    "    manual_win.geometry(\"420x330\")\n",
    "\n",
    "    back_btn = Button(manual_win, text=\"‚¨Ö Back\", bg=\"#f0ad4e\", command=manual_win.destroy)\n",
    "    back_btn.place(x=10, y=5)\n",
    "\n",
    "    Label(manual_win, text=\"üìñ User Manual\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(\n",
    "        manual_win,\n",
    "        text=(\n",
    "            \"üì∑ This app uses your webcam to detect facial emotions in real-time\\n\"\n",
    "            \"‚ñ∂ Press 'q' to exit the emotion detection window\\n\"\n",
    "            \"üíæ Press 'w' to capture and save the current frame\\n\"\n",
    "            \"üîç Emotions are displayed on the screen using DeepFace analysis\\n\"\n",
    "            \"üß† Uses advanced deep learning for accurate emotion prediction\"\n",
    "        ),\n",
    "        font=(\"Helvetica\", 11),\n",
    "        justify=\"left\"\n",
    "    ).pack(padx=15, pady=10)\n",
    "\n",
    "def show_benefits():\n",
    "    benefits_win = Toplevel(root)\n",
    "    benefits_win.title(\"App Benefits\")\n",
    "    benefits_win.geometry(\"420x300\")\n",
    "\n",
    "    back_btn = Button(benefits_win, text=\"‚¨Ö Back\", bg=\"#5bc0de\", command=benefits_win.destroy)\n",
    "    back_btn.place(x=10, y=5)\n",
    "\n",
    "    Label(benefits_win, text=\"‚ú® App Benefits\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(\n",
    "        benefits_win,\n",
    "        text=(\n",
    "            \"üòä Real-time facial emotion detection\\n\"\n",
    "            \"üñ•Ô∏è Simple and user-friendly interface\\n\"\n",
    "            \"ü§ñ Powered by deep learning and AI (DeepFace)\\n\"\n",
    "            \"üìä Displays dominant emotion with overlay\\n\"\n",
    "            \"üñºÔ∏è Option to capture snapshots during detection\\n\"\n",
    "            \"üéØ Useful for emotion-aware applications and analysis\\n\"\n",
    "            \"üí° Great for research and educational purposes\"\n",
    "        ),\n",
    "        font=(\"Helvetica\", 11),\n",
    "        justify=\"left\"\n",
    "    ).pack(padx=15, pady=10)\n",
    "\n",
    "def confirm_exit():\n",
    "    root.destroy()\n",
    "\n",
    "def detect_emotions():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        dominant_emotion = result[0]['dominant_emotion']\n",
    "\n",
    "        cv2.putText(frame, dominant_emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Real-Time Emotion Detection\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('w'):\n",
    "            cv2.imwrite(\"captured_emotion.png\", frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Real-Time Face Emotion Recognition\")\n",
    "root.geometry(\"400x220\")\n",
    "\n",
    "menu_bar = Menu(root)\n",
    "root.config(menu=menu_bar)\n",
    "menu_bar.add_command(label=\"User Manual\", command=show_manual)\n",
    "menu_bar.add_command(label=\"App Benefits\", command=show_benefits)\n",
    "\n",
    "back_button = tk.Button(root, text=\"‚¨Ö Back\", bg=\"#d9534f\", command=root.quit)\n",
    "back_button.place(x=10, y=5)\n",
    "\n",
    "label = tk.Label(root, text=\"Real-Time Face Emotion Recognition\", font=(\"Helvetica\", 14, \"bold\"))\n",
    "label.pack(pady=30)\n",
    "\n",
    "start_button = tk.Button(root, text=\"Open Camera for Prediction\", font=(\"Helvetica\", 12), bg=\"#5cb85c\", fg=\"white\", command=detect_emotions)\n",
    "start_button.pack(pady=10)\n",
    "\n",
    "exit_button = tk.Button(root, text=\"Exit\", font=(\"Helvetica\", 12), bg=\"#0275d8\", fg=\"white\", command=confirm_exit)\n",
    "exit_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3091a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Face emotion recognition app\n",
    "import tkinter as tk\n",
    "from tkinter import Toplevel, Label, Menu, Button\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "import glob\n",
    "\n",
    "# === Emoji & Save Paths ===\n",
    "emoji_dir = r\"C:\\Users\\moham\\Downloads\\Emotion_Recognition_Project\\Real-time-Facial-Emotion-Recognition-using-OpenCV-and-Deepface\\Emoji\"\n",
    "save_path = r\"C:\\Users\\moham\\Downloads\\Emotion_Captures\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# === Load emoji gifs and icons ===\n",
    "emoji_gifs = {}\n",
    "emoji_icons = {}\n",
    "for path in glob.glob(os.path.join(emoji_dir, \"*.gif\")):\n",
    "    emotion = os.path.splitext(os.path.basename(path))[0].lower()\n",
    "    emoji_gifs[emotion] = [cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR) for frame in imageio.mimread(path)]\n",
    "\n",
    "for path in glob.glob(os.path.join(emoji_dir, \"*.png\")):\n",
    "    emotion = os.path.splitext(os.path.basename(path))[0].lower()\n",
    "    emoji_icons[emotion] = cv2.resize(cv2.imread(path), (24, 24))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "def show_manual():\n",
    "    manual_win = Toplevel(root)\n",
    "    manual_win.title(\"User Manual\")\n",
    "    manual_win.geometry(\"420x330\")\n",
    "\n",
    "    Button(manual_win, text=\"‚¨Ö Back\", bg=\"#f0ad4e\", command=manual_win.destroy).place(x=10, y=5)\n",
    "\n",
    "    Label(manual_win, text=\"üìñ User Manual\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(\n",
    "        manual_win,\n",
    "        text=(\n",
    "            \"üì∑ This app uses your webcam to detect facial emotions in real-time\\n\"\n",
    "            \"‚ñ∂ Press 'q' to exit the emotion detection window\\n\"\n",
    "            \"üíæ Press 'w' to capture and save the current frame\\n\"\n",
    "            \"üîç Emotions are displayed on the screen using DeepFace analysis\\n\"\n",
    "            \"üß† Uses advanced deep learning for accurate emotion prediction\"\n",
    "        ),\n",
    "        font=(\"Helvetica\", 11),\n",
    "        justify=\"left\"\n",
    "    ).pack(padx=15, pady=10)\n",
    "\n",
    "def show_benefits():\n",
    "    benefits_win = Toplevel(root)\n",
    "    benefits_win.title(\"App Benefits\")\n",
    "    benefits_win.geometry(\"420x300\")\n",
    "\n",
    "    Button(benefits_win, text=\"‚¨Ö Back\", bg=\"#5bc0de\", command=benefits_win.destroy).place(x=10, y=5)\n",
    "\n",
    "    Label(benefits_win, text=\"‚ú® App Benefits\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(\n",
    "        benefits_win,\n",
    "        text=(\n",
    "            \"üòä Real-time facial emotion detection\\n\"\n",
    "            \"üñ•Ô∏è Simple and user-friendly interface\\n\"\n",
    "            \"ü§ñ Powered by deep learning and AI (DeepFace)\\n\"\n",
    "            \"üìä Displays dominant emotion with overlay\\n\"\n",
    "            \"üñºÔ∏è Option to capture snapshots during detection\\n\"\n",
    "            \"üéØ Useful for emotion-aware applications and analysis\\n\"\n",
    "            \"üí° Great for research and educational purposes\"\n",
    "        ),\n",
    "        font=(\"Helvetica\", 11),\n",
    "        justify=\"left\"\n",
    "    ).pack(padx=15, pady=10)\n",
    "\n",
    "def confirm_exit():\n",
    "    root.destroy()\n",
    "\n",
    "def detect_emotions():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    emoji_frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "        face_count = len(faces)\n",
    "\n",
    "        dominant_emotion = \"No Face\"\n",
    "        emotion_scores = None\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_roi = rgb[y:y + h, x:x + w]\n",
    "\n",
    "            try:\n",
    "                result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                dominant_emotion = result[0]['dominant_emotion'].lower()\n",
    "                emotion_scores = result[0]['emotion']\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, dominant_emotion.upper(), (x, y - 10), font, 0.7, (0, 255, 0), 2)\n",
    "            except Exception as e:\n",
    "                print(\"[WARNING] Emotion detection error:\", e)\n",
    "            break  # Only process the first face\n",
    "\n",
    "        # === LEFT PANEL CREATION ===\n",
    "        panel_width = 240\n",
    "        panel = np.ones((frame.shape[0], panel_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Top emoji gif\n",
    "        if dominant_emotion in emoji_gifs:\n",
    "            emoji_frames = emoji_gifs[dominant_emotion]\n",
    "            emoji_img = emoji_frames[emoji_frame_index % len(emoji_frames)]\n",
    "            emoji_frame_index += 1\n",
    "            emoji_resized = cv2.resize(emoji_img, (144, 144))\n",
    "            panel[10:10 + 144, 48:48 + 144] = emoji_resized\n",
    "            cv2.putText(panel, dominant_emotion.upper(), (65, 170), font, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        # Bar chart with emotion values\n",
    "        if emotion_scores:\n",
    "            sorted_emotions = sorted(emotion_scores.items(), key=lambda item: -item[1])\n",
    "            bar_start_y = 200\n",
    "            bar_height = 18\n",
    "            spacing = 6\n",
    "            max_bar_width = 100\n",
    "\n",
    "            for idx, (emo, val) in enumerate(sorted_emotions):\n",
    "                y = bar_start_y + idx * (bar_height + spacing)\n",
    "                if emo in emoji_icons:\n",
    "                    panel[y:y + 24, 10:34] = emoji_icons[emo]\n",
    "\n",
    "                bar_len = int((val / 100) * max_bar_width)\n",
    "                cv2.rectangle(panel, (40, y + 5), (140, y + 5 + bar_height), (230, 230, 230), -1)\n",
    "                cv2.rectangle(panel, (40, y + 5), (40 + bar_len, y + 5 + bar_height), (0, 100, 255), -1)\n",
    "                cv2.putText(panel, f\"{emo.upper()} : {int(val)}%\", (145, y + 20), font, 0.4, (0, 0, 0), 1)\n",
    "\n",
    "        cv2.putText(frame, f\"Faces: {face_count}\", (10, 30), font, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        combined = np.hstack((panel, frame))\n",
    "        cv2.imshow(\"Real-Time Emotion Detection\", combined)\n",
    "\n",
    "        key = cv2.waitKey(100) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('w'):\n",
    "            save_file = os.path.join(save_path, f\"emotion_{dominant_emotion}.png\")\n",
    "            cv2.imwrite(save_file, frame)\n",
    "            print(f\"[INFO] Saved: {save_file}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# === GUI SETUP ===\n",
    "root = tk.Tk()\n",
    "root.title(\"Real-Time Face Emotion Recognition\")\n",
    "root.geometry(\"400x220\")\n",
    "\n",
    "menu_bar = Menu(root)\n",
    "root.config(menu=menu_bar)\n",
    "menu_bar.add_command(label=\"User Manual\", command=show_manual)\n",
    "menu_bar.add_command(label=\"App Benefits\", command=show_benefits)\n",
    "\n",
    "tk.Button(root, text=\"‚¨Ö Back\", bg=\"#d9534f\", command=root.quit).place(x=10, y=5)\n",
    "\n",
    "tk.Label(root, text=\"Real-Time Face Emotion Recognition\", font=(\"Helvetica\", 14, \"bold\")).pack(pady=30)\n",
    "\n",
    "tk.Button(root, text=\"Open Camera for Prediction\", font=(\"Helvetica\", 12), bg=\"#5cb85c\", fg=\"white\", command=detect_emotions).pack(pady=10)\n",
    "tk.Button(root, text=\"Exit\", font=(\"Helvetica\", 12), bg=\"#0275d8\", fg=\"white\", command=confirm_exit).pack(pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b5a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## Final interface app\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import Toplevel, Label, Menu, Button, PhotoImage\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "import glob\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "# === Emoji & Save Paths ===\n",
    "emoji_dir = r\"C:\\Users\\moham\\Downloads\\Emotion_Recognition_Project\\Real-time-Facial-Emotion-Recognition-using-OpenCV-and-Deepface\\Emoji\"\n",
    "save_path = r\"C:\\Users\\moham\\Downloads\\Emotion_Captures\"\n",
    "image_path = r\"C:\\Users\\moham\\Downloads\\Emotion_Recognition_Project\\Real-time-Facial-Emotion-Recognition-using-OpenCV-and-Deepface\\image\\face_emotion.png\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# === Load emoji gifs and icons ===\n",
    "emoji_gifs = {}\n",
    "emoji_icons = {}\n",
    "for path in glob.glob(os.path.join(emoji_dir, \"*.gif\")):\n",
    "    emotion = os.path.splitext(os.path.basename(path))[0].lower()\n",
    "    emoji_gifs[emotion] = [cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR) for frame in imageio.mimread(path)]\n",
    "\n",
    "for path in glob.glob(os.path.join(emoji_dir, \"*.png\")):\n",
    "    emotion = os.path.splitext(os.path.basename(path))[0].lower()\n",
    "    emoji_icons[emotion] = cv2.resize(cv2.imread(path), (24, 24))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "video_writer = None\n",
    "recording = False\n",
    "\n",
    "# === GUI SETUP ===\n",
    "root = tk.Tk()\n",
    "root.title(\"Real-Time Face Emotion Recognition\")\n",
    "root.geometry(\"900x500\")\n",
    "\n",
    "# === LEFT PANEL ===\n",
    "left_frame = tk.Frame(root, width=400, bg=\"white\")\n",
    "left_frame.pack(side=\"left\", fill=\"both\")\n",
    "\n",
    "Label(left_frame, text=\"Real-Time Face Emotion Recognition\", font=(\"Helvetica\", 14, \"bold\"), bg=\"white\").pack(pady=30)\n",
    "\n",
    "camera_icon = PhotoImage(file=\"camera_icon.png\") if os.path.exists(\"camera_icon.png\") else None\n",
    "capture_icon = PhotoImage(file=\"capture_icon.png\") if os.path.exists(\"capture_icon.png\") else None\n",
    "record_icon = PhotoImage(file=\"record_icon.png\") if os.path.exists(\"record_icon.png\") else None\n",
    "exit_icon = PhotoImage(file=\"exit_icon.png\") if os.path.exists(\"exit_icon.png\") else None\n",
    "\n",
    "Button(left_frame, text=\"üòÄ Start Prediction\", font=(\"Helvetica\", 12), bg=\"#5cb85c\", fg=\"white\", command=lambda: threading.Thread(target=detect_emotions).start()).pack(pady=10)\n",
    "Button(left_frame, text=\"‚ùå Exit\", font=(\"Helvetica\", 12), bg=\"#0275d8\", fg=\"white\", command=root.quit).pack(pady=10)\n",
    "\n",
    "# === RIGHT PANEL ===\n",
    "right_frame = tk.Frame(root)\n",
    "right_frame.pack(side=\"right\", fill=\"both\", expand=True)\n",
    "try:\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((480, 480))\n",
    "    img_tk = ImageTk.PhotoImage(img)\n",
    "    Label(right_frame, image=img_tk).pack()\n",
    "except:\n",
    "    Label(right_frame, text=\"Image could not be loaded\", font=(\"Helvetica\", 14)).pack(pady=200)\n",
    "\n",
    "# === MENUBAR ===\n",
    "menu_bar = Menu(root)\n",
    "root.config(menu=menu_bar)\n",
    "menu_bar.add_command(label=\"User Manual\", command=lambda: show_manual())\n",
    "menu_bar.add_command(label=\"App Benefits\", command=lambda: show_benefits())\n",
    "\n",
    "def show_manual():\n",
    "    manual_win = Toplevel(root)\n",
    "    manual_win.title(\"User Manual\")\n",
    "    manual_win.geometry(\"420x330\")\n",
    "    Button(manual_win, text=\"‚¨Ö Back\", bg=\"#f0ad4e\", command=manual_win.destroy).place(x=10, y=5)\n",
    "    Label(manual_win, text=\"üìñ User Manual\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(manual_win, text=(\n",
    "        \"üì∑ This app uses your webcam to detect facial emotions in real-time\\n\"\n",
    "        \"‚ñ∂ Press 'q' to exit the emotion detection window\\n\"\n",
    "        \"üíæ Press 'w' to capture and save the current frame\\n\"\n",
    "        \"üîç Emotions are displayed on the screen using DeepFace analysis\\n\"\n",
    "        \"üß† Uses advanced deep learning for accurate emotion prediction\"\n",
    "    ), font=(\"Helvetica\", 11), justify=\"left\").pack(padx=15, pady=10)\n",
    "\n",
    "def show_benefits():\n",
    "    benefits_win = Toplevel(root)\n",
    "    benefits_win.title(\"App Benefits\")\n",
    "    benefits_win.geometry(\"420x300\")\n",
    "    Button(benefits_win, text=\"‚¨Ö Back\", bg=\"#5bc0de\", command=benefits_win.destroy).place(x=10, y=5)\n",
    "    Label(benefits_win, text=\"‚ú® App Benefits\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(benefits_win, text=(\n",
    "        \"üòä Real-time facial emotion detection\\n\"\n",
    "        \"üñ•Ô∏è Simple and user-friendly interface\\n\"\n",
    "        \"ü§ñ Powered by deep learning and AI (DeepFace)\\n\"\n",
    "        \"üìä Displays dominant emotion with overlay\\n\"\n",
    "        \"üñºÔ∏è Option to capture snapshots during detection\\n\"\n",
    "        \"üéØ Useful for emotion-aware applications and analysis\\n\"\n",
    "        \"üí° Great for research and educational purposes\"\n",
    "    ), font=(\"Helvetica\", 11), justify=\"left\").pack(padx=15, pady=10)\n",
    "\n",
    "def toggle_video_record():\n",
    "    global recording\n",
    "    recording = not recording\n",
    "\n",
    "def capture_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        file_name = f\"capture_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "        path = os.path.join(save_path, file_name)\n",
    "        cv2.imwrite(path, frame)\n",
    "        cv2.imshow(\"Captured Image\", frame)\n",
    "        cv2.waitKey(1500)\n",
    "        cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "def detect_emotions():\n",
    "    global video_writer, recording\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    emoji_frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "        face_count = len(faces)\n",
    "\n",
    "        dominant_emotion = \"No Face\"\n",
    "        emotion_scores = None\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_roi = rgb[y:y + h, x:x + w]\n",
    "\n",
    "            try:\n",
    "                result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                dominant_emotion = result[0]['dominant_emotion'].lower()\n",
    "                emotion_scores = result[0]['emotion']\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, dominant_emotion.upper(), (x, y - 10), font, 0.7, (0, 255, 0), 2)\n",
    "            except Exception as e:\n",
    "                print(\"[WARNING] Emotion detection error:\", e)\n",
    "            break  # Only process first face\n",
    "\n",
    "        panel_width = 240\n",
    "        panel = np.ones((frame.shape[0], panel_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        if dominant_emotion in emoji_gifs:\n",
    "            emoji_frames = emoji_gifs[dominant_emotion]\n",
    "            emoji_img = emoji_frames[emoji_frame_index % len(emoji_frames)]\n",
    "            emoji_frame_index += 1\n",
    "            emoji_resized = cv2.resize(emoji_img, (144, 144))\n",
    "            panel[10:10 + 144, 48:48 + 144] = emoji_resized\n",
    "            cv2.putText(panel, dominant_emotion.upper(), (65, 170), font, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        if emotion_scores:\n",
    "            sorted_emotions = sorted(emotion_scores.items(), key=lambda item: -item[1])\n",
    "            bar_start_y = 200\n",
    "            bar_height = 18\n",
    "            spacing = 6\n",
    "            max_bar_width = 100\n",
    "\n",
    "            for idx, (emo, val) in enumerate(sorted_emotions):\n",
    "                y = bar_start_y + idx * (bar_height + spacing)\n",
    "                if emo in emoji_icons:\n",
    "                    panel[y:y + 24, 10:34] = emoji_icons[emo]\n",
    "                bar_len = int((val / 100) * max_bar_width)\n",
    "                cv2.rectangle(panel, (40, y + 5), (140, y + 5 + bar_height), (230, 230, 230), -1)\n",
    "                cv2.rectangle(panel, (40, y + 5), (40 + bar_len, y + 5 + bar_height), (0, 100, 255), -1)\n",
    "                cv2.putText(panel, f\"{emo.upper()} : {int(val)}%\", (145, y + 20), font, 0.4, (0, 0, 0), 1)\n",
    "\n",
    "        if recording:\n",
    "            if video_writer is None:\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "                filename = f\"video_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.avi\"\n",
    "                filepath = os.path.join(save_path, filename)\n",
    "                video_writer = cv2.VideoWriter(filepath, fourcc, 10, (frame.shape[1], frame.shape[0]))\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        cv2.putText(frame, f\"Faces: {face_count}\", (10, 30), font, 0.7, (255, 255, 255), 2)\n",
    "        combined = np.hstack((panel, frame))\n",
    "        cv2.imshow(\"Real-Time Emotion Detection\", combined)\n",
    "\n",
    "        key = cv2.waitKey(100) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "        video_writer = None\n",
    "    recording = False\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
