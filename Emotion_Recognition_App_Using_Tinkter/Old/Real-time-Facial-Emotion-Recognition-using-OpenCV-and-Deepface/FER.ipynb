{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b08194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1.Imports & Global Setup\n",
    "import tkinter as tk\n",
    "from tkinter import Toplevel, Label, Menu, Button, PhotoImage\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "import glob\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "# === Emoji & Save Paths ===\n",
    "emoji_dir = r\"C:\\Users\\moham\\Downloads\\Emotion_Recognition_Project\\Real-time-Facial-Emotion-Recognition-using-OpenCV-and-Deepface\\Emoji\"\n",
    "save_path = r\"C:\\Users\\moham\\Downloads\\Emotion_Captures\"\n",
    "image_path = r\"C:\\Users\\moham\\Downloads\\Emotion_Recognition_Project\\Real-time-Facial-Emotion-Recognition-using-OpenCV-and-Deepface\\image\\face_emotion.png\"\n",
    "os.makedirs(save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee05c69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2.Load Emojis, Fonts, GUI Setup\n",
    "# === Load emoji gifs and icons ===\n",
    "emoji_gifs = {}\n",
    "emoji_icons = {}\n",
    "for path in glob.glob(os.path.join(emoji_dir, \"*.gif\")):\n",
    "    emotion = os.path.splitext(os.path.basename(path))[0].lower()\n",
    "    emoji_gifs[emotion] = [cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR) for frame in imageio.mimread(path)]\n",
    "\n",
    "for path in glob.glob(os.path.join(emoji_dir, \"*.png\")):\n",
    "    emotion = os.path.splitext(os.path.basename(path))[0].lower()\n",
    "    emoji_icons[emotion] = cv2.resize(cv2.imread(path), (24, 24))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "video_writer = None\n",
    "recording = False\n",
    "\n",
    "# === GUI SETUP ===\n",
    "root = tk.Tk()\n",
    "root.title(\"Real-Time Face Emotion Recognition\")\n",
    "root.geometry(\"900x500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf03db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: GUI Layout & Menu\n",
    "# === LEFT PANEL ===\n",
    "left_frame = tk.Frame(root, width=400, bg=\"white\")\n",
    "left_frame.pack(side=\"left\", fill=\"both\")\n",
    "\n",
    "Label(left_frame, text=\"Real-Time Face Emotion Recognition\", font=(\"Helvetica\", 14, \"bold\"), bg=\"white\").pack(pady=30)\n",
    "\n",
    "camera_icon = PhotoImage(file=\"camera_icon.png\") if os.path.exists(\"camera_icon.png\") else None\n",
    "capture_icon = PhotoImage(file=\"capture_icon.png\") if os.path.exists(\"capture_icon.png\") else None\n",
    "record_icon = PhotoImage(file=\"record_icon.png\") if os.path.exists(\"record_icon.png\") else None\n",
    "exit_icon = PhotoImage(file=\"exit_icon.png\") if os.path.exists(\"exit_icon.png\") else None\n",
    "\n",
    "Button(left_frame, text=\"üòÄ Start Prediction\", font=(\"Helvetica\", 12), bg=\"#5cb85c\", fg=\"white\", command=lambda: threading.Thread(target=detect_emotions).start()).pack(pady=10)\n",
    "Button(left_frame, text=\"‚ùå Exit\", font=(\"Helvetica\", 12), bg=\"#0275d8\", fg=\"white\", command=root.quit).pack(pady=10)\n",
    "\n",
    "# === RIGHT PANEL ===\n",
    "right_frame = tk.Frame(root)\n",
    "right_frame.pack(side=\"right\", fill=\"both\", expand=True)\n",
    "try:\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((480, 480))\n",
    "    img_tk = ImageTk.PhotoImage(img)\n",
    "    Label(right_frame, image=img_tk).pack()\n",
    "except:\n",
    "    Label(right_frame, text=\"Image could not be loaded\", font=(\"Helvetica\", 14)).pack(pady=200)\n",
    "\n",
    "# === MENUBAR ===\n",
    "menu_bar = Menu(root)\n",
    "root.config(menu=menu_bar)\n",
    "menu_bar.add_command(label=\"User Manual\", command=lambda: show_manual())\n",
    "menu_bar.add_command(label=\"App Benefits\", command=lambda: show_benefits())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328e10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: Helper Functions (manuals, detection, etc.)\n",
    "def show_manual():\n",
    "    manual_win = Toplevel(root)\n",
    "    manual_win.title(\"User Manual\")\n",
    "    manual_win.geometry(\"420x330\")\n",
    "    Button(manual_win, text=\"‚¨Ö Back\", bg=\"#f0ad4e\", command=manual_win.destroy).place(x=10, y=5)\n",
    "    Label(manual_win, text=\"üìñ User Manual\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(manual_win, text=(\n",
    "        \"üì∑ This app uses your webcam to detect facial emotions in real-time\\n\"\n",
    "        \"‚ñ∂ Press 'q' to exit the emotion detection window\\n\"\n",
    "        \"üíæ Press 'w' to capture and save the current frame\\n\"\n",
    "        \"üîç Emotions are displayed on the screen using DeepFace analysis\\n\"\n",
    "        \"üß† Uses advanced deep learning for accurate emotion prediction\"\n",
    "    ), font=(\"Helvetica\", 11), justify=\"left\").pack(padx=15, pady=10)\n",
    "\n",
    "def show_benefits():\n",
    "    benefits_win = Toplevel(root)\n",
    "    benefits_win.title(\"App Benefits\")\n",
    "    benefits_win.geometry(\"420x300\")\n",
    "    Button(benefits_win, text=\"‚¨Ö Back\", bg=\"#5bc0de\", command=benefits_win.destroy).place(x=10, y=5)\n",
    "    Label(benefits_win, text=\"‚ú® App Benefits\", font=(\"Helvetica\", 13, \"bold\")).pack(pady=10)\n",
    "    Label(benefits_win, text=(\n",
    "        \"üòä Real-time facial emotion detection\\n\"\n",
    "        \"üñ•Ô∏è Simple and user-friendly interface\\n\"\n",
    "        \"ü§ñ Powered by deep learning and AI (DeepFace)\\n\"\n",
    "        \"üìä Displays dominant emotion with overlay\\n\"\n",
    "        \"üñºÔ∏è Option to capture snapshots during detection\\n\"\n",
    "        \"üéØ Useful for emotion-aware applications and analysis\\n\"\n",
    "        \"üí° Great for research and educational purposes\"\n",
    "    ), font=(\"Helvetica\", 11), justify=\"left\").pack(padx=15, pady=10)\n",
    "\n",
    "def toggle_video_record():\n",
    "    global recording\n",
    "    recording = not recording\n",
    "\n",
    "def capture_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        file_name = f\"capture_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "        path = os.path.join(save_path, file_name)\n",
    "        cv2.imwrite(path, frame)\n",
    "        cv2.imshow(\"Captured Image\", frame)\n",
    "        cv2.waitKey(1500)\n",
    "        cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "def detect_emotions():\n",
    "    global video_writer, recording\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    emoji_frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "        face_count = len(faces)\n",
    "\n",
    "        dominant_emotion = \"No Face\"\n",
    "        emotion_scores = None\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_roi = rgb[y:y + h, x:x + w]\n",
    "\n",
    "            try:\n",
    "                result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                dominant_emotion = result[0]['dominant_emotion'].lower()\n",
    "                emotion_scores = result[0]['emotion']\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, dominant_emotion.upper(), (x, y - 10), font, 0.7, (0, 255, 0), 2)\n",
    "            except Exception as e:\n",
    "                print(\"[WARNING] Emotion detection error:\", e)\n",
    "            break\n",
    "\n",
    "        panel_width = 240\n",
    "        panel = np.ones((frame.shape[0], panel_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        if dominant_emotion in emoji_gifs:\n",
    "            emoji_frames = emoji_gifs[dominant_emotion]\n",
    "            emoji_img = emoji_frames[emoji_frame_index % len(emoji_frames)]\n",
    "            emoji_frame_index += 1\n",
    "            emoji_resized = cv2.resize(emoji_img, (144, 144))\n",
    "            panel[10:10 + 144, 48:48 + 144] = emoji_resized\n",
    "            cv2.putText(panel, dominant_emotion.upper(), (65, 170), font, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        if emotion_scores:\n",
    "            sorted_emotions = sorted(emotion_scores.items(), key=lambda item: -item[1])\n",
    "            bar_start_y = 200\n",
    "            bar_height = 18\n",
    "            spacing = 6\n",
    "            max_bar_width = 100\n",
    "\n",
    "            for idx, (emo, val) in enumerate(sorted_emotions):\n",
    "                y = bar_start_y + idx * (bar_height + spacing)\n",
    "                if emo in emoji_icons:\n",
    "                    panel[y:y + 24, 10:34] = emoji_icons[emo]\n",
    "                bar_len = int((val / 100) * max_bar_width)\n",
    "                cv2.rectangle(panel, (40, y + 5), (140, y + 5 + bar_height), (230, 230, 230), -1)\n",
    "                cv2.rectangle(panel, (40, y + 5), (40 + bar_len, y + 5 + bar_height), (0, 100, 255), -1)\n",
    "                cv2.putText(panel, f\"{emo.upper()} : {int(val)}%\", (145, y + 20), font, 0.4, (0, 0, 0), 1)\n",
    "\n",
    "        if recording:\n",
    "            if video_writer is None:\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "                filename = f\"video_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.avi\"\n",
    "                filepath = os.path.join(save_path, filename)\n",
    "                video_writer = cv2.VideoWriter(filepath, fourcc, 10, (frame.shape[1], frame.shape[0]))\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        cv2.putText(frame, f\"Faces: {face_count}\", (10, 30), font, 0.7, (255, 255, 255), 2)\n",
    "        combined = np.hstack((panel, frame))\n",
    "        cv2.imshow(\"Real-Time Emotion Detection\", combined)\n",
    "\n",
    "        key = cv2.waitKey(100) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "        video_writer = None\n",
    "    recording = False\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a062dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    " ##Cell 5: Run App (Canvas Launch)\n",
    "# === Launch the GUI (canvas window opens here) ===\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773f9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
